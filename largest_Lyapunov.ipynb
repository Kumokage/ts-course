{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Largest Lyapunov exponent calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic map\n",
    "def logistic_map(n=2**15, r=4, x0=.4):\n",
    "    x = np.zeros(n)\n",
    "    x[0] = x0\n",
    "    for i in range(n-1):\n",
    "        x[i+1] = r*x[i]*(1-x[i])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Henon map\n",
    "def henon_map(n=1000000, a=1.4, b=0.3, x0=.4):\n",
    "    x = np.zeros(n)\n",
    "    x[0] = x0\n",
    "    for i in range(1, len(x)):\n",
    "        x[i] = 1 - a * x[i-1] ** 2 + b * x[i-1]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Lorenz ts\n",
    "def lorenz_ts(N=None):\n",
    "    x = np.array([])\n",
    "    i = 0\n",
    "    with open(\"lorenz.txt\") as f:\n",
    "        for line in f:\n",
    "            x = np.append(x, float(line))\n",
    "            i += 1\n",
    "            if N is not None and i == N:\n",
    "                break\n",
    "    return x\n",
    "\n",
    "sine_data = np.sin(np.arange(0,1000,.01))\n",
    "\n",
    "lorenz = lorenz_ts()\n",
    "logistic = logistic_map()\n",
    "henon = henon_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[inf,  0.,  0.,  0.,  0.],\n",
       "       [ 0., inf,  0.,  0.,  0.],\n",
       "       [ 0.,  0., inf,  0.,  0.],\n",
       "       [ 0.,  0.,  0., inf,  0.],\n",
       "       [ 0.,  0.,  0.,  0., inf]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.ones(5) * np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([1, 2, 3, 4])\n",
    "test[test > 2] = 2\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def largest_exponent(series: np.array, J: int, m: int, t: float, trajectory_len: int = 20):\n",
    "    # compute shape values\n",
    "    N = series.shape[0]\n",
    "    M = N - (m-1) * J\n",
    "\n",
    "    # reconstruct with lag algorithm\n",
    "    x = np.zeros((M, m))\n",
    "    for i in range(M):\n",
    "        indexes = np.ones(m) * i + np.arange(m) * J\n",
    "        x[i] = series[indexes.astype(int)]\n",
    "    \n",
    "    # Find nearest neighbor\n",
    "    distances = euclidean_distances(x)\n",
    "    neighbors = np.argmin(distances + np.diag(np.ones(M) * np.inf), axis=1)\n",
    "\n",
    "    # mean rate of separation\n",
    "    y = np.zeros(trajectory_len)\n",
    "    for i in range(trajectory_len):\n",
    "        neighbors_i = neighbors[:M-i] + i\n",
    "        neighbors_i[neighbors_i >= M] = (M - 1)\n",
    "        separation = distances[(np.arange(M - i) + i, neighbors_i)]\n",
    "        separation = separation[neighbors[:M-i] + i < M]\n",
    "        separation = separation[separation != 0]\n",
    "        \n",
    "        if separation.shape[0] == 0:\n",
    "            y[i] = np.inf\n",
    "        else:\n",
    "            y[i] = np.log(separation).mean() / t\n",
    "\n",
    "    y = y[np.isfinite(y)]\n",
    "    slope, _ = np.polyfit(np.arange(1, len(y) + 1), y, 1)\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066\n"
     ]
    }
   ],
   "source": [
    "print(f\"{largest_exponent(logistic[:501], J=1, m=2, t=1, trajectory_len=48):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs):\n",
    "    \"\"\"**Time-delay Embedding of a Signal**\n",
    "\n",
    "    Time-delay embedding is one of the key concept of complexity science. It is based on the idea\n",
    "    that a dynamical system can be described by a vector of numbers, called its *'state'*, that\n",
    "    aims to provide a complete description of the system at some point in time. The set of all\n",
    "    possible states is called the *'state space'*.\n",
    "\n",
    "    Takens's (1981) embedding theorem suggests that a sequence of measurements of a dynamic system\n",
    "    includes in itself all the information required to completely reconstruct the state space.\n",
    "    Time-delay embedding attempts to identify the state *s* of the system at some time *t* by\n",
    "    searching the past history of observations for similar states, and, by studying the evolution\n",
    "    of similar states, infer information about the future of the system.\n",
    "\n",
    "    **Attractors**\n",
    "\n",
    "    How to visualize the dynamics of a system? A sequence of state values over time is called a\n",
    "    trajectory. Depending on the system, different trajectories can evolve to a common subset of\n",
    "    state space called an attractor. The presence and behavior of attractors gives intuition about\n",
    "    the underlying dynamical system. We can visualize the system and its attractors by plotting the\n",
    "    trajectory of many different initial state values and numerically integrating them to\n",
    "    approximate their continuous time evolution on discrete computers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : Union[list, np.array, pd.Series]\n",
    "        The signal (i.e., a time series) in the form of a vector of values. Can also be a string,\n",
    "        such as ``\"lorenz\"`` (Lorenz attractor), ``\"rossler\"`` (RÃ¶ssler attractor), or\n",
    "        ``\"clifford\"`` (Clifford attractor) to obtain a pre-defined attractor.\n",
    "    delay : int\n",
    "        Time delay (often denoted *Tau* :math:`\\\\tau`, sometimes referred to as *lag*) in samples.\n",
    "        See :func:`complexity_delay` to estimate the optimal value for this parameter.\n",
    "    dimension : int\n",
    "        Embedding Dimension (*m*, sometimes referred to as *d* or *order*). See\n",
    "        :func:`complexity_dimension` to estimate the optimal value for this parameter.\n",
    "    show : bool\n",
    "        Plot the reconstructed attractor. See :func:`complexity_attractor` for details.\n",
    "    **kwargs\n",
    "        Other arguments to be passed to :func:`complexity_attractor`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Embedded time-series, of shape ``length - (dimension - 1) * delay``\n",
    "\n",
    "    See Also\n",
    "    ------------\n",
    "    complexity_delay, complexity_dimension, complexity_attractor\n",
    "\n",
    "    Examples\n",
    "    ---------\n",
    "    **Example 1**: Understanding the output\n",
    "\n",
    "    .. ipython\n",
    "\n",
    "      import neurokit2 as nk\n",
    "\n",
    "      # Basic example\n",
    "      signal = [1, 2, 3, 2.5, 2.0, 1.5]\n",
    "      embedded = nk.complexity_embedding(signal, delay = 2, dimension = 2)\n",
    "      embedded\n",
    "\n",
    "    The first columns contains the beginning of the signal, and the second column contains the\n",
    "    values at *t+2*.\n",
    "\n",
    "    **Example 2**: 2D, 3D, and \"4D\" Attractors. Note that 3D attractors are slow to plot.\n",
    "\n",
    "    .. ipython\n",
    "\n",
    "      # Artifical example\n",
    "      signal = nk.signal_simulate(duration=4, sampling_rate=200, frequency=5, noise=0.01)\n",
    "\n",
    "      @savefig p_complexity_embedding1.png scale=100%\n",
    "      embedded = nk.complexity_embedding(signal, delay=50, dimension=2, show=True)\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    .. ipython\n",
    "\n",
    "      @savefig p_complexity_embedding2.png scale=100%\n",
    "      embedded = nk.complexity_embedding(signal, delay=50, dimension=3, show=True)\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    .. ipython\n",
    "\n",
    "      @savefig p_complexity_embedding3.png scale=100%\n",
    "      embedded = nk.complexity_embedding(signal, delay=50, dimension=4, show=True)\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    In the last 3D-attractor, the 4th dimension is represented by the color.\n",
    "\n",
    "    **Example 3**: Attractor of heart rate\n",
    "\n",
    "      ecg = nk.ecg_simulate(duration=60*4, sampling_rate=200)\n",
    "      peaks, _ = nk.ecg_peaks(ecg, sampling_rate=200)\n",
    "      signal = nk.ecg_rate(peaks, sampling_rate=200, desired_length=len(ecg))\n",
    "\n",
    "      @savefig p_complexity_embedding4.png scale=100%\n",
    "      embedded = nk.complexity_embedding(signal, delay=250, dimension=2, show=True)\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    References\n",
    "    -----------\n",
    "    * Gautama, T., Mandic, D. P., & Van Hulle, M. M. (2003, April). A differential entropy based\n",
    "      method for determining the optimal embedding parameters of a signal. In 2003 IEEE\n",
    "      International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.\n",
    "      (ICASSP'03). (Vol. 6, pp. VI-29). IEEE.\n",
    "    * Takens, F. (1981). Detecting strange attractors in turbulence. In Dynamical systems and\n",
    "      turbulence, Warwick 1980 (pp. 366-381). Springer, Berlin, Heidelberg.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(signal)\n",
    "\n",
    "    # Sanity checks\n",
    "    if dimension * delay > N:\n",
    "        raise ValueError(\n",
    "            \"NeuroKit error: complexity_embedding(): dimension * delay should be lower than\",\n",
    "            \" the length of the signal.\",\n",
    "        )\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"NeuroKit error: complexity_embedding(): 'delay' has to be at least 1.\")\n",
    "\n",
    "    Y = np.zeros((dimension, N - (dimension - 1) * delay))\n",
    "    for i in range(dimension):\n",
    "        Y[i] = signal[i * delay : i * delay + Y.shape[1]]\n",
    "    embedded = Y.T\n",
    "\n",
    "    return embedded\n",
    "\n",
    "\n",
    "\n",
    "def _complexity_lyapunov_rosenstein(\n",
    "    signal, delay=1, dimension=2, separation=1, len_trajectory=20, show=False, **kwargs\n",
    "):\n",
    "    # 1. Check that sufficient data points are available\n",
    "    # Minimum length required to find single orbit vector\n",
    "    min_len = (dimension - 1) * delay + 1\n",
    "    # We need len_trajectory orbit vectors to follow a complete trajectory\n",
    "    min_len += len_trajectory - 1\n",
    "    # we need tolerance * 2 + 1 orbit vectors to find neighbors for each\n",
    "    min_len += separation * 2 + 1\n",
    "\n",
    "    # Embedding\n",
    "    embedded = complexity_embedding(signal, delay=delay, dimension=dimension)\n",
    "    m = len(embedded)\n",
    "\n",
    "    # Construct matrix with pairwise distances between vectors in orbit\n",
    "    dists = euclidean_distances(embedded)\n",
    "\n",
    "    for i in range(m):\n",
    "        # Exclude indices within separation\n",
    "        dists[i, max(0, i - separation) : i + separation + 1] = np.inf\n",
    "\n",
    "    # Find indices of nearest neighbours\n",
    "    ntraj = m - len_trajectory + 1\n",
    "    min_dist_indices = np.argmin(\n",
    "        dists[:ntraj, :ntraj], axis=1\n",
    "    )  # exclude last few indices\n",
    "    min_dist_indices = min_dist_indices.astype(int)\n",
    "\n",
    "    # Follow trajectories of neighbour pairs for len_trajectory data points\n",
    "    trajectories = np.zeros(len_trajectory)\n",
    "    for k in range(len_trajectory):\n",
    "        divergence = dists[(np.arange(ntraj) + k, min_dist_indices + k)]\n",
    "        dist_nonzero = np.where(divergence != 0)[0]\n",
    "        if len(dist_nonzero) == 0:\n",
    "            trajectories[k] = -np.inf\n",
    "        else:\n",
    "            # Get average distances of neighbour pairs along the trajectory\n",
    "            trajectories[k] = np.mean(np.log(divergence[dist_nonzero]))\n",
    "\n",
    "    divergence_rate = trajectories[np.isfinite(trajectories)]\n",
    "\n",
    "    # LLE obtained by least-squares fit to average line\n",
    "    slope, intercept = np.polyfit(\n",
    "        np.arange(1, len(divergence_rate) + 1), divergence_rate, 1\n",
    "    )\n",
    "\n",
    "    # Store info\n",
    "    parameters = {\n",
    "        \"Trajectory_Length\": len_trajectory,\n",
    "        \"Divergence_Rate\": divergence_rate,\n",
    "    }\n",
    "\n",
    "    if show is True:\n",
    "        plt.plot(np.arange(1, len(divergence_rate) + 1), divergence_rate)\n",
    "        plt.axline(\n",
    "            (0, intercept), slope=slope, color=\"orange\", label=\"Least-squares Fit\"\n",
    "        )\n",
    "        plt.ylabel(\"Divergence Rate\")\n",
    "        plt.title(f\"Largest Lyapunov Exponent (slope of the line) = {slope:.3f}\")\n",
    "        plt.legend()\n",
    "\n",
    "    return slope, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10081046006089485,\n",
       " {'Trajectory_Length': 10,\n",
       "  'Divergence_Rate': array([-3.20168663, -2.78025586, -2.63505089, -2.49701362, -2.28973965,\n",
       "         -2.14620778, -2.13358299, -2.1820157 , -2.19914652, -2.19424653])})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_complexity_lyapunov_rosenstein(lorenz[:5001], 11, 5, len_trajectory=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
